{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced scraping with Scrapy\n",
    "\n",
    "## What you will learn in this course ðŸ§ðŸ§\n",
    "\n",
    "As you learned how to parse HTML pages, it is now time to go to the next level and scrape websites automatically. The best way to do so is by using spiders from Scrapy. In this course, we'll learn:\n",
    "\n",
    "* How to create basic crawlers \n",
    "* Target specific tags and attributes in a webpage \n",
    "* Follow links to scrap multiple pages\n",
    "* Simulate user log-in\n",
    "* Run multiple crawlers at the same time\n",
    "* Avoid being banned from websites\n",
    "\n",
    "If Scrapy isn't installed yet in your environment, just execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Scrapy\n",
      "  Downloading Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Scrapy) (4.6.3)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\berenger\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Scrapy) (59.4.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Scrapy) (3.4.8)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.5.0-py3-none-any.whl (10 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Scrapy) (5.4.0)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Scrapy) (20.0.1)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-22.2.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.2.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from cryptography>=2.0->Scrapy) (1.14.6)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->Scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->Scrapy) (21.2.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->Scrapy) (3.10.0.2)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2\n",
      "  Downloading twisted_iocpsupport-1.0.2-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (2.26.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from tldextract->Scrapy) (3.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->Scrapy) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\berenger\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.0.4)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=12559 sha256=f147a55ccca64860b1db7c19233ae3cb4ba0b16c44e9eb00dfca158a1dc4dad5\n",
      "  Stored in directory: c:\\users\\berenger\\appdata\\local\\pip\\cache\\wheels\\3c\\31\\7f\\d7d7b5f0b9bad841ed856138ff0c5ee2bf2e04dbeb413097c8\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: w3lib, pyasn1, cssselect, twisted-iocpsupport, requests-file, pyasn1-modules, parsel, itemadapter, incremental, hyperlink, constantly, Automat, Twisted, tldextract, service-identity, queuelib, PyDispatcher, protego, itemloaders, Scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Scrapy-2.6.1 Twisted-22.2.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.5.0 itemloaders-1.0.4 parsel-1.6.0 protego-0.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.2.0 twisted-iocpsupport-1.0.2 w3lib-1.22.0\n"
     ]
    }
   ],
   "source": [
    "# Add '!' only if you are running this command on a notebook \n",
    "## It tells Jupyter that the command should be interpreted as bash command\n",
    "# !pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first spider ðŸ•·ï¸ðŸ•·ï¸\n",
    "\n",
    "Basically, Scrapy works with *Spiders* that describe the successive steps necessary to get the data you're interested in at a given url. To make a scraping engine, you will need to:\n",
    "\n",
    "- declare your own class that inherits from `Scrapy.Spider`,\n",
    "- declare two attributes: the `name` of your crawler and the `url` at which you will start crawling,\n",
    "- declare a `parse` method with an argument called `response` (which represents the variable containing the HTML response at the `url` you just defined). This method will describe all the steps required to extract the desired data from the HTML elements, by using CSS selectors.\n",
    "\n",
    "Let's begin with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomQuoteSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"randomquote\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/random',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the first <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quote = response.css('div.quote')\n",
    "        return {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('span small.author::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to declare a `CrawlerProcess` that will run the spider and save the results in a `json` file (called a \"FEED\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 12:37:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-03-31 12:37:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-03-31 12:37:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-03-31 12:37:20 [scrapy.extensions.telnet] INFO: Telnet Password: f2fa036b535d9386\n",
      "2022-03-31 12:37:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-03-31 12:37:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-03-31 12:37:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-03-31 12:37:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-03-31 12:37:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-03-31 12:37:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-31 12:37:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-03-31 12:37:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-03-31 12:37:21 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: src/1_randomquote.json\n",
      "2022-03-31 12:37:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 945,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.358611,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 3, 31, 10, 37, 21, 558474),\n",
      " 'httpcompression/response_bytes': 2429,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 3, 31, 10, 37, 21, 199863)}\n",
      "2022-03-31 12:37:21 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"1_randomquote.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(RandomQuoteSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: Scrapy is not made to run multiple independant crawlers in one script. For this reason, please restart your notebook's kernel before declaring a new `CrawlerProcess` (otherwise an error will be raised and the crawling won't run).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple items per page ðŸ›ï¸ðŸ›ï¸\n",
    "\n",
    "Let's see an example where we parse multiple elements with a `for` loop and python's `yield` instruction (see appendix 1 of this lecture for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of all the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 12:34:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-03-31 12:34:09 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'QuotesSpider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25076/3007672939.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Start the crawling using the spider you defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQuotesSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QuotesSpider' is not defined"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"2_quotes.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following pagination links ðŸ“„ðŸ“„\n",
    "\n",
    "The example below shows how to use links to iterate over multiple pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesMultipleSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotesmultiplepages\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Select the NEXT button and store it in next_page\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-12 16:37:02 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-01-12 16:37:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2021-01-12 16:37:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-01-12 16:37:02 [scrapy.extensions.telnet] INFO: Telnet Password: f5985b1abb976ab8\n",
      "2021-01-12 16:37:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-01-12 16:37:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-01-12 16:37:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-01-12 16:37:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-01-12 16:37:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-01-12 16:37:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-01-12 16:37:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-01-12 16:37:05 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-01-12 16:37:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-01-12 16:37:05 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/3_quotesmultiplepages.json\n",
      "2021-01-12 16:37:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2866,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 23083,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'elapsed_time_seconds': 2.558098,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 1, 12, 16, 37, 5, 516190),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 87691264,\n",
      " 'memusage/startup': 87691264,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2021, 1, 12, 16, 37, 2, 958092)}\n",
      "2021-01-12 16:37:05 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"3_quotesmultiplepages.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesMultipleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication on a website ðŸ”ðŸ”\n",
    "\n",
    "A very useful feature of Scrapy: you can simulate automatic authentication!\n",
    "\n",
    "This can be done by using `scrapy.FormRequest.from_response()` to send a post request with some your login/password to the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesLogin(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"login\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['http://quotes.toscrape.com/login']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'username': 'john', 'password': 'secret'},\n",
    "\n",
    "            # Function to be called once logged in\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_login(self, response):\n",
    "\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.after_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-12 16:38:25 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-01-12 16:38:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2021-01-12 16:38:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-01-12 16:38:25 [scrapy.extensions.telnet] INFO: Telnet Password: b10086537d39bd1e\n",
      "2021-01-12 16:38:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-01-12 16:38:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-01-12 16:38:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-01-12 16:38:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-01-12 16:38:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-01-12 16:38:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-01-12 16:38:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-01-12 16:38:28 [root] INFO: No next page. Terminating crawling process.\n",
      "2021-01-12 16:38:28 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-01-12 16:38:28 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/4_quotesauthentication.json\n",
      "2021-01-12 16:38:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5404,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 11,\n",
      " 'downloader/request_method_count/POST': 1,\n",
      " 'downloader/response_bytes': 26193,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 2.888981,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 1, 12, 16, 38, 28, 757237),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 87715840,\n",
      " 'memusage/startup': 87715840,\n",
      " 'request_depth_max': 10,\n",
      " 'response_received_count': 11,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2021, 1, 12, 16, 38, 25, 868256)}\n",
      "2021-01-12 16:38:28 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"4_quotesauthentication.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesLogin)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple spiders simultaneously ðŸ•¸ï¸ ðŸ•·ï¸\n",
    "\n",
    "As stated before, it's not possible to run multiple crawlers in the same python script. But if you'd like to crawl different pages in parallel, this can be done by declaring multiple spiders!\n",
    "\n",
    "Then you just have to call `process.crawl()` as many times as you need, by passing your different spiders, as we illustrate below. The results will all be stored as a list of JSON data in the same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpiderPage1(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of all <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "            \n",
    "\n",
    "class QuotesSpiderPage2(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-12 16:40:23 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-01-12 16:40:23 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2021-01-12 16:40:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-01-12 16:40:23 [scrapy.extensions.telnet] INFO: Telnet Password: dea89b0c2fb90da5\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-01-12 16:40:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-01-12 16:40:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-01-12 16:40:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-01-12 16:40:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2021-01-12 16:40:23 [scrapy.extensions.telnet] INFO: Telnet Password: cb6d8a11fb084d26\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-01-12 16:40:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-01-12 16:40:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-01-12 16:40:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-01-12 16:40:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2021-01-12 16:40:24 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-01-12 16:40:24 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/5_quotesmultiplespiders.json\n",
      "2021-01-12 16:40:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2204,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.406941,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 1, 12, 16, 40, 24, 278192),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 20,\n",
      " 'memusage/max': 87740416,\n",
      " 'memusage/startup': 87740416,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 1, 12, 16, 40, 23, 871251)}\n",
      "2021-01-12 16:40:24 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2021-01-12 16:40:24 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-01-12 16:40:24 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/5_quotesmultiplespiders.json\n",
      "2021-01-12 16:40:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 3085,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.414772,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 1, 12, 16, 40, 24, 306425),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 15,\n",
      " 'memusage/max': 87740416,\n",
      " 'memusage/startup': 87740416,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2021, 1, 12, 16, 40, 23, 891653)}\n",
      "2021-01-12 16:40:24 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"5_quotesmultiplespiders.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesSpiderPage1)\n",
    "process.crawl(QuotesSpiderPage2)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid being banned: autothrottle ðŸš«ðŸš«\n",
    "\n",
    "The more scraping you're doing the more requests you make. If websites are well protected, they might ban you because you exceeded requests limitations. \n",
    "\n",
    "You may avoid that by delaying the number of requests automatically thanks to the `AutoThrottle` extension. \n",
    "\n",
    "As stated in the documentation, `AutoThrottle` extension is designed to: \n",
    "\n",
    "- *Be nicer to sites instead of using default download delay of zero.*\n",
    "- *Automatically adjust Scrapy to the optimum crawling speed, so the user doesnâ€™t have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.*\n",
    "\n",
    "To use autothrottle, it's as simple as adding `\"AUTOTHROTTLE_ENABLED\": True` to your crawler's settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesMultipleSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotesmultiplepages\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:31:19 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:31:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:31:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:31:19 [scrapy.extensions.telnet] INFO: Telnet Password: 86b452237fed67cf\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:31:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:31:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:31:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:31:32 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-11-13 10:31:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:31:32 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/6_quotesautothrottle.json\n",
      "2020-11-13 10:31:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2866,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 23094,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'elapsed_time_seconds': 12.883837,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 31, 32, 254774),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75186176,\n",
      " 'memusage/startup': 75186176,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 31, 19, 370937)}\n",
      "2020-11-13 10:31:32 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"6_quotesautothrottle.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\": True  # AutoThrottle Here!\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesMultipleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - What is Yield keyword for? ðŸ’\n",
    "\n",
    "You might have noticed that we used the `yield` keyword in Scrapy which could be quite new and confusing. Technically speaking it is called a *generator*.\n",
    "\n",
    "In a nutshell, `yield` is a very useful keyword to return a data collection without taking up too much machine's memory. \n",
    "\n",
    "Let's check out with an example. Let's take two functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function with return keyword\n",
    "def return_list(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        a_list[i] *= 2\n",
    "    return a_list\n",
    "\n",
    "# Function with yield keyword\n",
    "def return_with_yield(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        yield a_list[i] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply these two functions to our `random_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Returns a list\n",
    "return_list(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object return_with_yield at 0x000001FCE2515270>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Function with yield\n",
    "return_with_yield(random_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example, `return_list` returned directly the full list. Whereas, in the second example, `return_with_yield` returned a `generator`. Generators are very cool because we haven't actually executed the loop. Therefore, we haven't spend too much computer memory. \n",
    "\n",
    "So let's say instead of a list of 10 items, you'd have one of 1000000 items, it would make a huge difference in terms of computing time. \n",
    "\n",
    "Now if you need to get the actual values of your generator, you can simply create a for loop or a comprehension list like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 0\n",
      "output 2\n",
      "output 4\n",
      "output 6\n",
      "output 8\n",
      "output 10\n",
      "output 12\n",
      "output 14\n",
      "output 16\n",
      "output 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a for loop will just print the output:\n",
    "for number in return_with_yield(random_list):\n",
    "    print(\"output\", number)\n",
    "\n",
    "# Using a comprehension list will create a list:\n",
    "[i for i in return_with_yield(random_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply need to yield from a list without doing any manipulation, you can use `yield from` instead of creating a loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2 - Crash course on XPath âš”ï¸\n",
    "\n",
    "In this lecture, you've learned how to use CSS selectors with Scrapy. Another way of scraping websites with Scrapy is by using XPaths.\n",
    "\n",
    "The best way to learn XPath is to follow this great tutorial from <a href=\"http://zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths\" target=\"_blank\">http://Zvon.org</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources ðŸ“šðŸ“š\n",
    "\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/index.html\" target=\"_blank\"> Scrapy Documentation </a>\n",
    "* <a href=\"https://docs.python.org/3/library/logging.html\" target=\"_blank\"> Logging</a>\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging\" target=\"_blank\">Logging in a scrapy</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
