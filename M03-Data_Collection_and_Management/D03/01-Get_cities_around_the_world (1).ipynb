{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxVLQcfegi2z"
   },
   "source": [
    "# Get Cities of The World Quality of Life Data\n",
    "\n",
    "In this exercise, we will try to get scoring information about the quality of life for several cities around the world. üåç\n",
    "\n",
    "For this exercise, we will be using the following API:\n",
    "\n",
    "- <a href=\"https://developers.teleport.org/api/getting_started/\" target=\"_blank\">Teleport</a>\n",
    "\n",
    "We will also need to use a website called RandomList.com that will give us a random cities around the world to get a scoring. \n",
    "\n",
    "Then we will store the data we got into an S3 Bucket! \n",
    "\n",
    "Quite a project, right? ü•µ\n",
    "\n",
    "ü•∞ü•∞ You'll learn a lot during this exercise ü•∞ü•∞ \n",
    "\n",
    "So let's go üí™üí™üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj9WjfGxICJI"
   },
   "source": [
    "## Part 1: Get data for 1 City \n",
    "\n",
    "To simplify this exercise, let's start by trying to scrape data for only 1 city: Paris. In another part, we'll try to get scores for 100 different cities.\n",
    "\n",
    "- Import the library called `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJOVO8rAduG5"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6vYEoJfIghv"
   },
   "source": [
    "* Check teleport's API, to find a way to search information on Paris. Especially, we would need its `geonameid`\n",
    "\n",
    "  * Here is the link for the documentation üëâüëâüëâ [Teleport API](https://developers.teleport.org/api/getting_started/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1576164583531,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "yJUETL9Fhdzf",
    "outputId": "6252424b-7cec-4931-8a14-c7a42e2b24c5"
   },
   "outputs": [],
   "source": [
    "cityParis = requests.get('https://api.teleport.org/api/cities/?search=Paris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSDu5yUfJJRq"
   },
   "source": [
    "‚ÑπÔ∏è‚ÑπÔ∏è You should get the following result ‚ÑπÔ∏è‚ÑπÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1576164789281,
     "user": {
      "displayName": "Antoine Krajnc",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mC4XzNDVGvURzl4T5duDbMr6bUdhkYkDul_37G0OA=s64",
      "userId": "08465960390418158788"
     },
     "user_tz": -60
    },
    "id": "LCckwB39lvPE",
    "outputId": "919402c2-9fdb-49fb-a811-39366a7d0283"
   },
   "outputs": [],
   "source": [
    "cityParis = cityParis.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GOvxg6AI-kt"
   },
   "source": [
    "* Now that you got the a list of search results, try to isolate Paris' `geonameid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.teleport.org/api/cities/geonameid:2988507/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityParis[\"_embedded\"][\"city:search-results\"][0][\"_links\"][\"city:item\"][\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQsO1QvZJbNj"
   },
   "source": [
    "* Use `requests` to get information about Paris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityParisID = requests.get('https://api.teleport.org/api/cities/geonameid:2988507/')\n",
    "cityParisID = cityParisID.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-n9wjJAJgiZ"
   },
   "source": [
    "* You should now be able to get Paris' quality of life scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityParisQualityOfLife = requests.get(cityParisID[\"_links\"][\"city:urban_area\"][\"href\"]+\"scores/\")\n",
    "cityParisQualityOfLife = cityParisQualityOfLife.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GstOo4oJnR1"
   },
   "source": [
    "* Use `Pandas` to create a DataFrame where you'll get all the scores for Paris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>name</th>\n",
       "      <th>score_out_of_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#f3c32c</td>\n",
       "      <td>Housing</td>\n",
       "      <td>3.5835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#f3d630</td>\n",
       "      <td>Cost of Living</td>\n",
       "      <td>3.6640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#f4eb33</td>\n",
       "      <td>Startups</td>\n",
       "      <td>9.2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#d2ed31</td>\n",
       "      <td>Venture Capital</td>\n",
       "      <td>7.5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#7adc29</td>\n",
       "      <td>Travel Connectivity</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     color                 name  score_out_of_10\n",
       "0  #f3c32c              Housing           3.5835\n",
       "1  #f3d630       Cost of Living           3.6640\n",
       "2  #f4eb33             Startups           9.2765\n",
       "3  #d2ed31      Venture Capital           7.5130\n",
       "4  #7adc29  Travel Connectivity          10.0000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lst_categories = [t for t in cityParisQualityOfLife[\"categories\"]]\n",
    "df = pd.DataFrame(columns=lst_categories[0].keys())\n",
    "\n",
    "for i in lst_categories:\n",
    "    df = df.append(i, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dojOEUeYJ3JD"
   },
   "source": [
    "* We now need to upload this DataFrame to S3. Let's first create a Boto3 session \n",
    "  * For the following, refer to the following documentation üëâüëâüëâ [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVnXNqpBx7wT"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGVdSWPiKA3K"
   },
   "source": [
    "* Now create a resource session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBXRVTKD0uuf"
   },
   "outputs": [],
   "source": [
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgzOgXnVKEa1"
   },
   "source": [
    "* Create a Bucket that you'll call `scoring-cities-in-the-world`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_9qH2LP0zJF"
   },
   "outputs": [],
   "source": [
    "bucket = s3.create_bucket(Bucket=\"jedha-bq-scoring-cities-in-the-world\", CreateBucketConfiguration={'LocationConstraint': 'eu-west-3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ik8xbgY1KjEo"
   },
   "source": [
    "* Use `Pandas` to export your DataFrame as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzw4SThH1XJR"
   },
   "outputs": [],
   "source": [
    "csv = df.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySj6lBOsKow5"
   },
   "source": [
    "* Use `put_object()` function to create an Object within the bucket you just created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VzDwONM2FGT"
   },
   "outputs": [],
   "source": [
    "put_object = bucket.put_object(Key=\"test.csv\", Body=csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRTXpA0xKwry"
   },
   "source": [
    "## Get Data For Several Cities \n",
    "\n",
    "üòâ Congrats ! üòâ You made it to the second part of the exercise. We now need more data to be able to compare them later. Let's try to find a way to get data for a lot more cities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lQ_1FlxLijw"
   },
   "source": [
    "* Go on to [this Wikipedia page](https://en.wikipedia.org/wiki/List_of_largest_cities). There you'll find a list of the world's largest cities.\n",
    "  * Use `scrapy` to scrape the city names directly from this page üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"wikipedia\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/wiki/List_of_largest_cities',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        city = response.css('tbody')\n",
    "        \n",
    "        for i in city:\n",
    "        \n",
    "            yield {\n",
    "                # 'text': i.css('a::text').getall()\n",
    "                'text': i.css('tr::attr(id)').getall()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-02 13:59:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-04-02 13:59:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-04-02 13:59:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-04-02 13:59:11 [scrapy.extensions.telnet] INFO: Telnet Password: 7a436e4acd98e934\n",
      "2022-04-02 13:59:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-04-02 13:59:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-04-02 13:59:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-04-02 13:59:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-04-02 13:59:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-04-02 13:59:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-04-02 13:59:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-04-02 13:59:12 [filelock] INFO: Lock 1807800703440 acquired on C:\\Users\\Berenger\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-04-02 13:59:12 [filelock] INFO: Lock 1807800701520 acquired on C:\\Users\\Berenger\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2022-04-02 13:59:12 [filelock] INFO: Lock 1807800701520 released on C:\\Users\\Berenger\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2022-04-02 13:59:12 [filelock] INFO: Lock 1807800703440 released on C:\\Users\\Berenger\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-04-02 13:59:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-04-02 13:59:13 [scrapy.extensions.feedexport] INFO: Stored json feed (4 items) in: wikipedia-city-name.json\n",
      "2022-04-02 13:59:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 263,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 57267,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.846657,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 4, 2, 11, 59, 13, 116732),\n",
      " 'httpcompression/response_bytes': 366667,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 4,\n",
      " 'log_count/INFO': 15,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 4, 2, 11, 59, 12, 270075)}\n",
      "2022-04-02 13:59:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"wikipedia-city-name.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir():\n",
    "        os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(WikipediaSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the json file with results from the crawling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wikipedia =  pd.read_json(\"wikipedia-city-name.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = wikipedia.iloc[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXoJfW5IM_8M"
   },
   "source": [
    "* Finally, create a loop that will go through each city, search for information and store it to your S3 bucket \n",
    "  * You might get some errors, definitely use the `try: \\ except:` structure \n",
    "  * (It's totally fine if you couldn't get info for all cities) üòåüòå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokyo done\n",
      "Delhi done\n",
      "Shanghai done\n",
      "S√£o_Paulo done\n",
      "Mexico_City done\n",
      "Cairo done\n",
      "Mumbai done\n",
      "Beijing done\n",
      "Couldn't find results for Dhaka\n",
      "Osaka done\n",
      "New_York_City done\n",
      "Couldn't find results for Karachi\n",
      "Buenos_Aires done\n",
      "Couldn't find results for Chongqing\n",
      "Istanbul done\n",
      "Couldn't find results for Kolkata\n",
      "Manila done\n",
      "Lagos done\n",
      "Rio_de_Janeiro done\n",
      "Couldn't find results for Tianjin\n",
      "Couldn't find results for Kinshasa\n",
      "Guangzhou done\n",
      "Los_Angeles done\n",
      "Moscow done\n",
      "Shenzhen done\n",
      "Couldn't find results for Lahore\n",
      "Bangalore done\n",
      "Paris done\n",
      "Bogot√° done\n",
      "Jakarta done\n",
      "Chennai done\n",
      "Lima done\n",
      "Bangkok done\n",
      "Seoul done\n",
      "Couldn't find results for Nagoya\n",
      "Hyderabad done\n",
      "London done\n",
      "Tehran done\n",
      "Chicago done\n",
      "Couldn't find results for Chengdu\n",
      "Couldn't find results for Nanjing\n",
      "Couldn't find results for Wuhan\n",
      "Ho_Chi_Minh_City done\n",
      "Couldn't find results for Luanda\n",
      "Couldn't find results for Ahmedabad\n",
      "Kuala_Lumpur done\n",
      "Couldn't find results for Xi'an\n",
      "Hong_Kong done\n",
      "Dongguan done\n",
      "Hangzhou done\n",
      "Foshan done\n",
      "Couldn't find results for Shenyang\n",
      "Riyadh done\n",
      "Couldn't find results for Baghdad\n",
      "Santiago done\n",
      "Couldn't find results for Surat\n",
      "Madrid done\n",
      "Suzhou done\n",
      "Couldn't find results for Pune\n",
      "Couldn't find results for Harbin\n",
      "Houston done\n",
      "Dallas done\n",
      "Toronto done\n",
      "Dar_es_Salaam done\n",
      "Miami done\n",
      "Couldn't find results for Belo_Horizonte\n",
      "Singapore done\n",
      "Philadelphia done\n",
      "Atlanta done\n",
      "Fukuoka done\n",
      "Couldn't find results for Khartoum\n",
      "Barcelona done\n",
      "Johannesburg done\n",
      "Saint_Petersburg done\n",
      "Couldn't find results for Qingdao\n",
      "Couldn't find results for Dalian\n",
      "Washington,_D.C. done\n",
      "Couldn't find results for Yangon\n",
      "Couldn't find results for Alexandria\n",
      "Couldn't find results for Jinan\n",
      "Guadalajara done\n"
     ]
    }
   ],
   "source": [
    "for i in city_list:\n",
    "    try:\n",
    "        current_city = requests.get('https://api.teleport.org/api/cities/?search='+i.replace(\"_\", \"%20\"))\n",
    "        current_city = current_city.json()\n",
    "        current_city_ID = current_city[\"_embedded\"][\"city:search-results\"][0][\"_links\"][\"city:item\"][\"href\"]\n",
    "        current_city_info = requests.get(current_city_ID)\n",
    "        current_city_info = current_city_info.json()\n",
    "        currentCityQualityOfLife = requests.get(current_city_info[\"_links\"][\"city:urban_area\"][\"href\"]+\"scores/\")\n",
    "        currentCityQualityOfLife = currentCityQualityOfLife.json()\n",
    "        lst_categories = [t for t in currentCityQualityOfLife[\"categories\"]]\n",
    "        df = pd.DataFrame(columns=lst_categories[0].keys())\n",
    "\n",
    "        for i2 in lst_categories:\n",
    "            df = df.append(i2, ignore_index=True)\n",
    "\n",
    "        df = df.rename(columns={'name': i})\n",
    "        csv = df.to_csv()\n",
    "        put_object = bucket.put_object(Key=i+\".csv\", Body=csv)\n",
    "        print (i+\" done\")\n",
    "    except:\n",
    "        print (\"Couldn't find results for \"+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpoUz9Z2NH0G"
   },
   "source": [
    "üéäüéäüéä Congratulations, You made it to the end of this exercise !! üéäüéäüéäüéä"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6b. Solution - Get Cities of The World Quality of Life Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
