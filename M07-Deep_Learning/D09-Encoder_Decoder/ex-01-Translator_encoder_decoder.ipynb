{"cells":[{"cell_type":"markdown","metadata":{"id":"5NFuUYKHABlD"},"source":["# Translations with ENcoder Decoder\n","\n","We'll see that with LSTMs and the Encoder Decoder framework, we can do some pretty powerful things like: *translators* ! Let's see how we can create a French > English translator with TensorFlow \n","\n","### Tips \n","\n","Don't take the whole dataset at the beginning for your experiments, just take 5000 or even 3000 sentences. This will allow you to iterate faster and avoid bugs simply related to your need for computing power.\n","\n","Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"em7GqFRv4nRZ"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1633714477653,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"2qUhyNPnhBtk","outputId":"0d6746ce-cae0-41da-8a06-2ae991989e22"},"outputs":[{"data":{"text/plain":["'2.8.0'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Import necessaries librairies\n","import pandas as pd\n","import numpy as np \n","import sklearn\n","import tensorflow_datasets as tfds\n","import tensorflow as tf \n","tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"3UoJ_qncuYKk"},"source":["## Importing data \n","\n","1. Load the data using the following url https://go.aws/38ECHUB you can read this using `pd.read_csv` with the `\"\\t\"` delimiter and `header=None`"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1335,"status":"ok","timestamp":1633714479320,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"U2-Sd6lq_8ax","outputId":"984135a3-b0ea-40f1-b756-6ca385d3ad13"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1\n","0   Go.        Va !\n","1   Hi.     Salut !\n","2  Run!     Cours !\n","3  Run!    Courez !\n","4  Wow!  Ça alors !"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Loading document txt function\n","df = pd.read_csv(\"https://go.aws/38ECHUB\", delimiter=\"\\t\", header=None)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"sJZCMavOoX3E"},"source":["2. Create an object `doc` containing the first 5000 rows from the file."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YrcFSfBZMuQ7"},"outputs":[],"source":["# Let's just take a sample of 5000 sentences to avoid slowness. \n","doc = df.iloc[:5000,:]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["5000"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["len(doc)"]},{"cell_type":"markdown","metadata":{"id":"eu-0BjwvoiGY"},"source":["3. In your opinion, are we going to need to lemmatize and remove stop words for a translation problem?"]},{"cell_type":"markdown","metadata":{"id":"ZUxnMkUOwIKb"},"source":["No because for stopwords are important to understand meaning."]},{"cell_type":"markdown","metadata":{"id":"_ha3hfzswnVd"},"source":["4. Add the word `<start>` to the beginning of each target sentence in order to create a new column named `padded_en`"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1633714479325,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"5AReCt5Zw5TF","outputId":"3abd6792-de20-4114-8292-64987a2e56c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Berenger\\AppData\\Local\\Temp/ipykernel_13496/2035661931.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  doc[\"padded_en\"] = doc.iloc[:,0].apply(lambda x: \"<start> \"+x)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1     padded_en\n","0   Go.        Va !   <start> Go.\n","1   Hi.     Salut !   <start> Hi.\n","2  Run!     Cours !  <start> Run!\n","3  Run!    Courez !  <start> Run!\n","4  Wow!  Ça alors !  <start> Wow!"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"padded_en\"] = doc.iloc[:,0].apply(lambda x: \"<start> \"+x)\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"0V_yusBAot6c"},"source":["5. Create two objects : `tokenizer_fr` and `tokenizer_en` that will be instances of the `tf.keras.preprocessing.text.Tokenizer` class. \n","\n","Be careful! Since we added a special token containing special characters, make sure you setup the tokenizers right so this token is well interpreted! (use the `filters` argument for example)."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"f5yAwku3eBro"},"outputs":[],"source":["tokenizer_fr = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')"]},{"cell_type":"markdown","metadata":{"id":"oNRgvH_co7Va"},"source":["6. Fit the tokenizers on the french, and **padded** english sentences respectively."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"OVxpAyw5eGAH"},"outputs":[],"source":["tokenizer_fr.fit_on_texts(doc.iloc[:,1])\n","tokenizer_en.fit_on_texts(doc[\"padded_en\"])"]},{"cell_type":"markdown","metadata":{"id":"CUojFPPwpCpo"},"source":["7. Create three new columns in your Dataframe for the encoded french, english, and padded english sentences."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":518},"executionInfo":{"elapsed":508,"status":"ok","timestamp":1633714479816,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"X2ueH5W8eTBZ","outputId":"9265158b-f53f-49ec-a255-793ca2841a11"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Berenger\\AppData\\Local\\Temp/ipykernel_13496/2664036674.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  doc[\"fr_indices\"] = tokenizer_fr.texts_to_sequences(doc.iloc[:,1])\n","C:\\Users\\Berenger\\AppData\\Local\\Temp/ipykernel_13496/2664036674.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  doc[\"en_indices\"] = tokenizer_en.texts_to_sequences(doc.iloc[:,0])\n","C:\\Users\\Berenger\\AppData\\Local\\Temp/ipykernel_13496/2664036674.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  doc[\"padded_en_indices\"] = tokenizer_en.texts_to_sequences(doc[\"padded_en\"])\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","      <th>fr_indices</th>\n","      <th>en_indices</th>\n","      <th>padded_en_indices</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>[36]</td>\n","      <td>[11]</td>\n","      <td>[1, 11]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>[404]</td>\n","      <td>[616]</td>\n","      <td>[1, 616]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1212]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1213]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>[22, 1214]</td>\n","      <td>[872]</td>\n","      <td>[1, 872]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1     padded_en  fr_indices en_indices padded_en_indices\n","0   Go.        Va !   <start> Go.        [36]       [11]           [1, 11]\n","1   Hi.     Salut !   <start> Hi.       [404]      [616]          [1, 616]\n","2  Run!     Cours !  <start> Run!      [1212]      [111]          [1, 111]\n","3  Run!    Courez !  <start> Run!      [1213]      [111]          [1, 111]\n","4  Wow!  Ça alors !  <start> Wow!  [22, 1214]      [872]          [1, 872]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"fr_indices\"] = tokenizer_fr.texts_to_sequences(doc.iloc[:,1])\n","doc[\"en_indices\"] = tokenizer_en.texts_to_sequences(doc.iloc[:,0])\n","doc[\"padded_en_indices\"] = tokenizer_en.texts_to_sequences(doc[\"padded_en\"])\n","\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"qFVz8F99yr7P"},"source":["8. We learned from the tutorial that the padded target sequences need to have the same length as the target sequences, so we will remove the last element of each padded target sequence (this will help us enforce teacher forcing)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1633714479817,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"9oYtlrlqzVBi","outputId":"dde0298e-cccc-4bda-fbb7-2b00290579bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Berenger\\AppData\\Local\\Temp/ipykernel_13496/554954058.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  doc[\"padded_en_indices_clean\"] = doc[\"padded_en_indices\"].apply(lambda x: x[:-1])\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>padded_en</th>\n","      <th>fr_indices</th>\n","      <th>en_indices</th>\n","      <th>padded_en_indices</th>\n","      <th>padded_en_indices_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Go.</td>\n","      <td>Va !</td>\n","      <td>&lt;start&gt; Go.</td>\n","      <td>[36]</td>\n","      <td>[11]</td>\n","      <td>[1, 11]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hi.</td>\n","      <td>Salut !</td>\n","      <td>&lt;start&gt; Hi.</td>\n","      <td>[404]</td>\n","      <td>[616]</td>\n","      <td>[1, 616]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Run!</td>\n","      <td>Cours !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1212]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Run!</td>\n","      <td>Courez !</td>\n","      <td>&lt;start&gt; Run!</td>\n","      <td>[1213]</td>\n","      <td>[111]</td>\n","      <td>[1, 111]</td>\n","      <td>[1]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Wow!</td>\n","      <td>Ça alors !</td>\n","      <td>&lt;start&gt; Wow!</td>\n","      <td>[22, 1214]</td>\n","      <td>[872]</td>\n","      <td>[1, 872]</td>\n","      <td>[1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0           1     padded_en  fr_indices en_indices padded_en_indices  \\\n","0   Go.        Va !   <start> Go.        [36]       [11]           [1, 11]   \n","1   Hi.     Salut !   <start> Hi.       [404]      [616]          [1, 616]   \n","2  Run!     Cours !  <start> Run!      [1212]      [111]          [1, 111]   \n","3  Run!    Courez !  <start> Run!      [1213]      [111]          [1, 111]   \n","4  Wow!  Ça alors !  <start> Wow!  [22, 1214]      [872]          [1, 872]   \n","\n","  padded_en_indices_clean  \n","0                     [1]  \n","1                     [1]  \n","2                     [1]  \n","3                     [1]  \n","4                     [1]  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["doc[\"padded_en_indices_clean\"] = doc[\"padded_en_indices\"].apply(lambda x: x[:-1])\n","doc.head()"]},{"cell_type":"markdown","metadata":{"id":"mSU_oz8WpShr"},"source":["9. It's rather difficult to work with sequences with variable length, use zero-padding to normalize the length of all the sequences in each category."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"UpNwsqpjO0TM"},"outputs":[],"source":["# Use of Keras to create token sequences of the same length\n","padded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], padding=\"post\")\n","padded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], padding=\"post\")\n","teacher_forcing_en = tf.keras.preprocessing.sequence.pad_sequences(doc[\"padded_en_indices_clean\"], padding=\"post\")"]},{"cell_type":"markdown","metadata":{"id":"BOje1lkApr4Q"},"source":["10. What are the shapes of the arrays you just created for the french, padded english, and english sentences?"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1633714479821,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"38el3B3z3btO","outputId":"31d254a0-43a5-4b05-a822-ba2e5c786475"},"outputs":[{"data":{"text/plain":["(5000, 10)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Visualization of the shape of one of the tensors\n","padded_fr_indices.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1633714479822,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"2qy5q9XSqQCB","outputId":"253c2a3b-8e80-4138-cc43-0bb7ab553de7"},"outputs":[{"data":{"text/plain":["(5000, 4)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["padded_en_indices.shape"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1633714479823,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"u9DUbG8D0AgG","outputId":"bfcdfc36-e6ea-4670-e5ec-15fb51859cb6"},"outputs":[{"data":{"text/plain":["(5000, 4)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["teacher_forcing_en.shape"]},{"cell_type":"markdown","metadata":{"id":"T0OTamAZ6gX4"},"source":["11. Use `sklearn` `train_test_split` function to divide your sample into train and validation sets."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5xTFcEtSLamK"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","en_train, en_val, fr_train, fr_val, teacher_train, teacher_val =  train_test_split(padded_en_indices,\n","                                                                                   padded_fr_indices,\n","                                                                                   teacher_forcing_en,\n","                                                                                   test_size=0.3)"]},{"cell_type":"markdown","metadata":{"id":"2A63yoDAsZJ0"},"source":["## MODEL\n","\n","Now it's time to code the model, thankfully you can largely base yourself off the code provided during the demo!"]},{"cell_type":"markdown","metadata":{"id":"krxpB96-64Rx"},"source":["1. Create the following variables:\n","* `n_embed` the number of dimensions you want for the embeddings output spaces\n","* `n_lstm` the number of units you want for the lstm layers\n","* `fr_len` the length of a french sentence\n","* `en_len` the length of an english or teacher forcing sentence\n","* `vocab_size_fr` the number of tokens in the french vocabulary\n","* `vocab_size_en` the number of tokens in the english vocabulary (based of the padded sequences so the `<start>` is included!"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LO6v2m4q1Z9V"},"outputs":[],"source":["# let's start by defining the number of units needed for the embedding and\n","# the lstm layers\n","\n","n_embed = 128\n","n_lstm = 64\n","fr_len = padded_fr_indices.shape[1]\n","en_len = padded_en_indices.shape[1]\n","vocab_size_fr = len(tokenizer_fr.word_index)\n","vocab_size_en = len(tokenizer_en.word_index)"]},{"cell_type":"markdown","metadata":{"id":"MJPChtoE71DK"},"source":["2. Set up the encoder\n","\n","This will work in the same way as the demo, just make sure the input dimension of the embedding is equal to the number of words in the french vocabulary +1 (for the zero-padding)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"BPGvJYwx1bqB"},"outputs":[],"source":["encoder_input = tf.keras.Input(shape=(fr_len))\n","encoder_embed = tf.keras.layers.Embedding(input_dim=vocab_size_fr+1, output_dim=n_embed)\n","encoder_lstm = tf.keras.layers.LSTM(n_lstm, return_state=True)\n","\n","encoder_embed_ouput = encoder_embed(encoder_input)\n","encoder_output = encoder_lstm(encoder_embed_ouput)\n","\n","encoder = tf.keras.Model(inputs = encoder_input, outputs = encoder_output)"]},{"cell_type":"markdown","metadata":{"id":"dblxRs9G8PQC"},"source":["3. Try the encoder on the french train data (using the call method)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1633714479828,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"Tc4TGKC02BFB","outputId":"6dbf83ea-098d-4334-ee89-6eb2707115b6"},"outputs":[{"data":{"text/plain":["[<tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.03127307, -0.0247299 , -0.01131879, ..., -0.02837454,\n","          0.0015578 , -0.01084446],\n","        [ 0.0109025 , -0.00775949, -0.00843466, ..., -0.02145365,\n","         -0.00233755, -0.00621878],\n","        [ 0.03008093, -0.02630412, -0.01079366, ..., -0.0263291 ,\n","          0.00063774, -0.01061366],\n","        ...,\n","        [ 0.02951849, -0.02076108, -0.01152032, ..., -0.02872757,\n","          0.00683473, -0.01130117],\n","        [ 0.03043274, -0.01922366, -0.01356811, ..., -0.03045454,\n","          0.00175662, -0.00974883],\n","        [ 0.02961827, -0.01894046, -0.01261697, ..., -0.03098983,\n","          0.00296335, -0.01341497]], dtype=float32)>,\n"," <tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.03127307, -0.0247299 , -0.01131879, ..., -0.02837454,\n","          0.0015578 , -0.01084446],\n","        [ 0.0109025 , -0.00775949, -0.00843466, ..., -0.02145365,\n","         -0.00233755, -0.00621878],\n","        [ 0.03008093, -0.02630412, -0.01079366, ..., -0.0263291 ,\n","          0.00063774, -0.01061366],\n","        ...,\n","        [ 0.02951849, -0.02076108, -0.01152032, ..., -0.02872757,\n","          0.00683473, -0.01130117],\n","        [ 0.03043274, -0.01922366, -0.01356811, ..., -0.03045454,\n","          0.00175662, -0.00974883],\n","        [ 0.02961827, -0.01894046, -0.01261697, ..., -0.03098983,\n","          0.00296335, -0.01341497]], dtype=float32)>,\n"," <tf.Tensor: shape=(3500, 64), dtype=float32, numpy=\n"," array([[ 0.06179565, -0.04881759, -0.0226705 , ..., -0.05685376,\n","          0.00309774, -0.02135796],\n","        [ 0.02159263, -0.01538625, -0.01690821, ..., -0.04331262,\n","         -0.00464152, -0.01229386],\n","        [ 0.05940554, -0.0519596 , -0.0216218 , ..., -0.05281112,\n","          0.0012695 , -0.02089683],\n","        ...,\n","        [ 0.0583721 , -0.04099299, -0.02308667, ..., -0.05758862,\n","          0.01359345, -0.02226564],\n","        [ 0.06014314, -0.03793339, -0.02721388, ..., -0.06110303,\n","          0.00349466, -0.01920287],\n","        [ 0.05854605, -0.03738891, -0.02528826, ..., -0.06215608,\n","          0.00589116, -0.02646169]], dtype=float32)>]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["encoder(fr_train)"]},{"cell_type":"markdown","metadata":{"id":"HZLS83o28Whq"},"source":["4. Set up the decoder\n","\n","This will work in the same way as the demo, just make sure the input dimension of the embedding is equal to the number of words in the french vocabulary +1 (for the zero-padding). The same goes for the last Dense layer!"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"zqgF-UNG2h5X"},"outputs":[],"source":["decoder_input = tf.keras.Input(shape=(en_len))\n","decoder_embed = tf.keras.layers.Embedding(input_dim=vocab_size_en+1, \n","                                          output_dim=n_embed)\n","decoder_lstm = tf.keras.layers.LSTM(n_lstm, return_sequences=True, return_state=True)\n","decoder_pred = tf.keras.layers.Dense(vocab_size_en+1, activation=\"softmax\")\n","\n","decoder_embed_output = decoder_embed(decoder_input) # teacher forcing happens here\n","# the decoder input is actually the padded target we created earlier, remember\n","# if target is: [91, 47, 89, 21, 62]\n","# the decoder input will be: [0, 91, 47, 89, 21]\n","decoder_lstm_output, _, _ = decoder_lstm(decoder_embed_output, initial_state=encoder_output[1:])\n","# in the step described above the decoder receives the encoder state as its\n","# initial state.\n","decoder_output = decoder_pred(decoder_lstm_output)\n","# then the dense layer will convert the vector representation for each element\n","# in the sequence into a probability distribution across all possible tokens\n","# in the vocabulary!\n","\n","decoder = tf.keras.Model(inputs = [encoder_input,decoder_input], outputs = decoder_output)\n","# all we need to do is put the model together using the input output framework!"]},{"cell_type":"markdown","metadata":{"id":"8d_9ixBl8miy"},"source":["5. Try the decoder on the french train data and the teacher forcing data"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1633714480190,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"4HkqPbEk2qk6","outputId":"0ddd04c4-d839-4299-9884-b6c7dc725906"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3500, 4, 1258), dtype=float32, numpy=\n","array([[[0.00079825, 0.00079616, 0.00079543, ..., 0.00078999,\n","         0.00079716, 0.00079411],\n","        [0.00079777, 0.00079625, 0.00079807, ..., 0.00079203,\n","         0.00079785, 0.00079019],\n","        [0.00079561, 0.00079708, 0.00079858, ..., 0.00079194,\n","         0.00079863, 0.00079213],\n","        [0.00079565, 0.00079878, 0.00079763, ..., 0.00079251,\n","         0.0007975 , 0.00079583]],\n","\n","       [[0.00079641, 0.00079744, 0.00079473, ..., 0.00079119,\n","         0.0007973 , 0.00079444],\n","        [0.00079652, 0.00079821, 0.00079695, ..., 0.00079222,\n","         0.00079664, 0.00079214],\n","        [0.00079917, 0.00079716, 0.00079847, ..., 0.00079343,\n","         0.00079604, 0.00079391],\n","        [0.00079894, 0.00079612, 0.00080092, ..., 0.00079451,\n","         0.00079332, 0.00079531]],\n","\n","       [[0.00079804, 0.00079618, 0.00079557, ..., 0.00078997,\n","         0.00079744, 0.00079443],\n","        [0.00079758, 0.00079625, 0.00079813, ..., 0.0007921 ,\n","         0.00079804, 0.00079044],\n","        [0.0007955 , 0.00079574, 0.00079853, ..., 0.00079226,\n","         0.00079649, 0.00079073],\n","        [0.00079584, 0.00079539, 0.00080119, ..., 0.00079411,\n","         0.00079422, 0.00079273]],\n","\n","       ...,\n","\n","       [[0.00079774, 0.00079622, 0.00079551, ..., 0.00079001,\n","         0.00079734, 0.00079398],\n","        [0.00079124, 0.00079686, 0.00079711, ..., 0.00078865,\n","         0.0007975 , 0.00079173],\n","        [0.00079295, 0.00079637, 0.00080047, ..., 0.00079128,\n","         0.00079501, 0.00079351],\n","        [0.000794  , 0.00079573, 0.0008026 , ..., 0.00079336,\n","         0.00079299, 0.00079496]],\n","\n","       [[0.00079827, 0.00079639, 0.000796  , ..., 0.00078976,\n","         0.00079761, 0.00079402],\n","        [0.00079619, 0.00079797, 0.00079636, ..., 0.00079028,\n","         0.0007979 , 0.0007917 ],\n","        [0.00079665, 0.00079738, 0.00079974, ..., 0.00079218,\n","         0.00079531, 0.00079358],\n","        [0.00079685, 0.00079659, 0.00080197, ..., 0.00079384,\n","         0.00079317, 0.00079497]],\n","\n","       [[0.00079781, 0.00079579, 0.00079571, ..., 0.00078982,\n","         0.00079709, 0.00079427],\n","        [0.00079679, 0.00079787, 0.0007947 , ..., 0.00079238,\n","         0.00079833, 0.0007946 ],\n","        [0.0007967 , 0.00079822, 0.00079534, ..., 0.00079213,\n","         0.00080025, 0.00079429],\n","        [0.00079689, 0.0007974 , 0.00079873, ..., 0.00079401,\n","         0.00079672, 0.00079534]]], dtype=float32)>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["decoder([fr_train,teacher_train])"]},{"cell_type":"markdown","metadata":{"id":"cR06rACR8uD6"},"source":["6. Set up the inference decoder\n","\n","The code here will be identical to the one from the demo except if you changed some naming conventions!"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"hp9wT_7e3HWO"},"outputs":[],"source":["decoder_state_input_h = tf.keras.Input(shape=(n_lstm,))\n","decoder_state_input_c = tf.keras.Input(shape=(n_lstm,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","# at the first step of the inference, these input will be respectively the\n","# hidden state and C state of the encoder model\n","# for following steps, they will become the hidden and C state from the decoder\n","# itself since the input sequence is unknown we will have to predict step by step\n","# using a loop\n","\n","decoder_input_inf = tf.keras.Input(shape=(1))\n","decoder_embed_output = decoder_embed(decoder_input_inf)\n","# the decoder input here is of shape 1 because we will feed the elements in the \n","# sequence one by one\n","\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_embed_output, initial_state=decoder_states_inputs)\n","# the lstm layer works in the same way, the output from the embedding is used\n","# and the decoder state is used as described above\n","\n","decoder_states = [state_h, state_c]\n","# we store the lstm states in a specific object as we'll have to use them as \n","# initial state for the next inference step\n","\n","decoder_outputs = decoder_pred(decoder_outputs)\n","# the lstm output is then converted to a probability distribution over the\n","# target vocabulary\n","\n","decoder_inf = tf.keras.Model(inputs = [decoder_input_inf, decoder_states_inputs], \n","                     outputs = [decoder_outputs, decoder_states])\n","# Finally we wrap up the model building by setting up the inputs and outputs"]},{"cell_type":"markdown","metadata":{"id":"5MVJbjVE8_HU"},"source":["7. Compile the decoder (the training version) using the appropriate loss and metric functions."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"aQwqIpRn3Xjg"},"outputs":[],"source":["decoder.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",")"]},{"cell_type":"markdown","metadata":{"id":"CdKFFI3i9JaL"},"source":["8. Train the decoder for 50 epochs, this should take 10 minutes. Is there overfitting ?"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78015,"status":"ok","timestamp":1633714558697,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"26EGWOcz3vgG","outputId":"54803b9a-1287-4a98-e849-99dfc209c605"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","110/110 [==============================] - 7s 26ms/step - loss: 4.9864 - sparse_categorical_accuracy: 0.3506 - val_loss: 4.1061 - val_sparse_categorical_accuracy: 0.3470\n","Epoch 2/50\n","110/110 [==============================] - 2s 17ms/step - loss: 3.7639 - sparse_categorical_accuracy: 0.3554 - val_loss: 3.6872 - val_sparse_categorical_accuracy: 0.3892\n","Epoch 3/50\n","110/110 [==============================] - 2s 17ms/step - loss: 3.5369 - sparse_categorical_accuracy: 0.3977 - val_loss: 3.5758 - val_sparse_categorical_accuracy: 0.3898\n","Epoch 4/50\n","110/110 [==============================] - 2s 16ms/step - loss: 3.4313 - sparse_categorical_accuracy: 0.4007 - val_loss: 3.5064 - val_sparse_categorical_accuracy: 0.3982\n","Epoch 5/50\n","110/110 [==============================] - 2s 17ms/step - loss: 3.3491 - sparse_categorical_accuracy: 0.4089 - val_loss: 3.4381 - val_sparse_categorical_accuracy: 0.4028\n","Epoch 6/50\n","110/110 [==============================] - 2s 16ms/step - loss: 3.2597 - sparse_categorical_accuracy: 0.4174 - val_loss: 3.3527 - val_sparse_categorical_accuracy: 0.4182\n","Epoch 7/50\n","110/110 [==============================] - 2s 17ms/step - loss: 3.1410 - sparse_categorical_accuracy: 0.4304 - val_loss: 3.2601 - val_sparse_categorical_accuracy: 0.4283\n","Epoch 8/50\n","110/110 [==============================] - 2s 17ms/step - loss: 3.0252 - sparse_categorical_accuracy: 0.4477 - val_loss: 3.1661 - val_sparse_categorical_accuracy: 0.4490\n","Epoch 9/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.9199 - sparse_categorical_accuracy: 0.4681 - val_loss: 3.0928 - val_sparse_categorical_accuracy: 0.4628\n","Epoch 10/50\n","110/110 [==============================] - 2s 16ms/step - loss: 2.8257 - sparse_categorical_accuracy: 0.4824 - val_loss: 3.0277 - val_sparse_categorical_accuracy: 0.4700\n","Epoch 11/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.7329 - sparse_categorical_accuracy: 0.4900 - val_loss: 2.9671 - val_sparse_categorical_accuracy: 0.4773\n","Epoch 12/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.6361 - sparse_categorical_accuracy: 0.5068 - val_loss: 2.9005 - val_sparse_categorical_accuracy: 0.4918\n","Epoch 13/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.5440 - sparse_categorical_accuracy: 0.5209 - val_loss: 2.8403 - val_sparse_categorical_accuracy: 0.5077\n","Epoch 14/50\n","110/110 [==============================] - 2s 16ms/step - loss: 2.4542 - sparse_categorical_accuracy: 0.5451 - val_loss: 2.7801 - val_sparse_categorical_accuracy: 0.5173\n","Epoch 15/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.3664 - sparse_categorical_accuracy: 0.5527 - val_loss: 2.7372 - val_sparse_categorical_accuracy: 0.5220\n","Epoch 16/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.2844 - sparse_categorical_accuracy: 0.5645 - val_loss: 2.6942 - val_sparse_categorical_accuracy: 0.5272\n","Epoch 17/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.2088 - sparse_categorical_accuracy: 0.5725 - val_loss: 2.6566 - val_sparse_categorical_accuracy: 0.5328\n","Epoch 18/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.1367 - sparse_categorical_accuracy: 0.5825 - val_loss: 2.6184 - val_sparse_categorical_accuracy: 0.5435\n","Epoch 19/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.0678 - sparse_categorical_accuracy: 0.5904 - val_loss: 2.5708 - val_sparse_categorical_accuracy: 0.5498\n","Epoch 20/50\n","110/110 [==============================] - 2s 17ms/step - loss: 2.0048 - sparse_categorical_accuracy: 0.5996 - val_loss: 2.5435 - val_sparse_categorical_accuracy: 0.5540\n","Epoch 21/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.9391 - sparse_categorical_accuracy: 0.6079 - val_loss: 2.5256 - val_sparse_categorical_accuracy: 0.5530\n","Epoch 22/50\n","110/110 [==============================] - 2s 18ms/step - loss: 1.8802 - sparse_categorical_accuracy: 0.6197 - val_loss: 2.5016 - val_sparse_categorical_accuracy: 0.5588\n","Epoch 23/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.8209 - sparse_categorical_accuracy: 0.6286 - val_loss: 2.4682 - val_sparse_categorical_accuracy: 0.5663\n","Epoch 24/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.7645 - sparse_categorical_accuracy: 0.6387 - val_loss: 2.4501 - val_sparse_categorical_accuracy: 0.5653\n","Epoch 25/50\n","110/110 [==============================] - 2s 16ms/step - loss: 1.7152 - sparse_categorical_accuracy: 0.6461 - val_loss: 2.4268 - val_sparse_categorical_accuracy: 0.5680\n","Epoch 26/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.6607 - sparse_categorical_accuracy: 0.6534 - val_loss: 2.4109 - val_sparse_categorical_accuracy: 0.5765\n","Epoch 27/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.6096 - sparse_categorical_accuracy: 0.6631 - val_loss: 2.4022 - val_sparse_categorical_accuracy: 0.5767\n","Epoch 28/50\n","110/110 [==============================] - 2s 16ms/step - loss: 1.5608 - sparse_categorical_accuracy: 0.6702 - val_loss: 2.3769 - val_sparse_categorical_accuracy: 0.5828\n","Epoch 29/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.5152 - sparse_categorical_accuracy: 0.6809 - val_loss: 2.3663 - val_sparse_categorical_accuracy: 0.5888\n","Epoch 30/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.4729 - sparse_categorical_accuracy: 0.6872 - val_loss: 2.3578 - val_sparse_categorical_accuracy: 0.5862\n","Epoch 31/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.4351 - sparse_categorical_accuracy: 0.6901 - val_loss: 2.3559 - val_sparse_categorical_accuracy: 0.5947\n","Epoch 32/50\n","110/110 [==============================] - 2s 16ms/step - loss: 1.3917 - sparse_categorical_accuracy: 0.6985 - val_loss: 2.3361 - val_sparse_categorical_accuracy: 0.5963\n","Epoch 33/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.3552 - sparse_categorical_accuracy: 0.7044 - val_loss: 2.3273 - val_sparse_categorical_accuracy: 0.5967\n","Epoch 34/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.3199 - sparse_categorical_accuracy: 0.7101 - val_loss: 2.3364 - val_sparse_categorical_accuracy: 0.5965\n","Epoch 35/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.2821 - sparse_categorical_accuracy: 0.7195 - val_loss: 2.3151 - val_sparse_categorical_accuracy: 0.6025\n","Epoch 36/50\n","110/110 [==============================] - 2s 16ms/step - loss: 1.2503 - sparse_categorical_accuracy: 0.7235 - val_loss: 2.3083 - val_sparse_categorical_accuracy: 0.6022\n","Epoch 37/50\n","110/110 [==============================] - 2s 16ms/step - loss: 1.2156 - sparse_categorical_accuracy: 0.7311 - val_loss: 2.2956 - val_sparse_categorical_accuracy: 0.6072\n","Epoch 38/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.1838 - sparse_categorical_accuracy: 0.7336 - val_loss: 2.3036 - val_sparse_categorical_accuracy: 0.6073\n","Epoch 39/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.1565 - sparse_categorical_accuracy: 0.7421 - val_loss: 2.2835 - val_sparse_categorical_accuracy: 0.6075\n","Epoch 40/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.1259 - sparse_categorical_accuracy: 0.7465 - val_loss: 2.2780 - val_sparse_categorical_accuracy: 0.6153\n","Epoch 41/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.0984 - sparse_categorical_accuracy: 0.7513 - val_loss: 2.2769 - val_sparse_categorical_accuracy: 0.6148\n","Epoch 42/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.0723 - sparse_categorical_accuracy: 0.7547 - val_loss: 2.2739 - val_sparse_categorical_accuracy: 0.6172\n","Epoch 43/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.0442 - sparse_categorical_accuracy: 0.7618 - val_loss: 2.2702 - val_sparse_categorical_accuracy: 0.6160\n","Epoch 44/50\n","110/110 [==============================] - 2s 17ms/step - loss: 1.0144 - sparse_categorical_accuracy: 0.7655 - val_loss: 2.2684 - val_sparse_categorical_accuracy: 0.6188\n","Epoch 45/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.9897 - sparse_categorical_accuracy: 0.7727 - val_loss: 2.2759 - val_sparse_categorical_accuracy: 0.6220\n","Epoch 46/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.9696 - sparse_categorical_accuracy: 0.7758 - val_loss: 2.2867 - val_sparse_categorical_accuracy: 0.6210\n","Epoch 47/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.9416 - sparse_categorical_accuracy: 0.7829 - val_loss: 2.2776 - val_sparse_categorical_accuracy: 0.6260\n","Epoch 48/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.9174 - sparse_categorical_accuracy: 0.7845 - val_loss: 2.2478 - val_sparse_categorical_accuracy: 0.6290\n","Epoch 49/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.8901 - sparse_categorical_accuracy: 0.7939 - val_loss: 2.2585 - val_sparse_categorical_accuracy: 0.6268\n","Epoch 50/50\n","110/110 [==============================] - 2s 17ms/step - loss: 0.8694 - sparse_categorical_accuracy: 0.7988 - val_loss: 2.2732 - val_sparse_categorical_accuracy: 0.6272\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x14b8604c880>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["decoder.fit(x=[fr_train, teacher_train], y=en_train,epochs=50, validation_data=([fr_val, teacher_val], en_val))"]},{"cell_type":"markdown","metadata":{"id":"-w3kOO6g9YO4"},"source":["9. Adapt the code from the demo to make some predictions on the validation data.\n","\n","Be careful, in the demo the starting index for the teacher forcing sequences was 0, what index is the starting point of the teacher forcing sequences now?\n","\n","Set up the first decoder input with the right dimension too!"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1633714558698,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"mO87iO6a5dlc","outputId":"3433439e-c825-44d9-ecce-c3ce3cfaa4a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["pred: [  9  44   4 410]\n","true: [  9 834   0   0]\n","\n","\n","pred: [ 94 388  56  32]\n","true: [ 94 388   0   0]\n","\n","\n","pred: [17 23  4 68]\n","true: [17 23  4  0]\n","\n","\n","pred: [241  53  75 118]\n","true: [241  53   0   0]\n","\n","\n","pred: [ 16   7 343  82]\n","true: [ 16 576   0   0]\n","\n","\n","pred: [ 3 46 53 53]\n","true: [  3 248   0   0]\n","\n","\n","pred: [ 2 76 36 11]\n","true: [  2 785   4   0]\n","\n","\n","pred: [70 98 45 12]\n","true: [143  92 283   0]\n","\n","\n","pred: [  8 149 860  39]\n","true: [  8 864  12   0]\n","\n","\n","pred: [  2 116 262 118]\n","true: [  2 710   0   0]\n","\n","\n"]}],"source":["enc_input = fr_val\n","#classic encoder input\n","\n","dec_input = tf.ones(shape=(len(fr_val),1))\n","# the first decoder input is the special token 0\n","\n","enc_out, state_h_inf, state_c_inf = encoder(enc_input)\n","# we compute once and for all the encoder output and the encoder\n","# h state and c state\n","\n","dec_state = [state_h_inf, state_c_inf]\n","# The encoder h state and c state will serve as initial states for the\n","# decoder\n","\n","pred = []  # we'll store the predictions in here\n","\n","# we loop over the expected length of the target, but actually the loop can run\n","# for as many steps as we wish, which is the advantage of the encoder decoder\n","# architecture\n","for i in range(en_len):\n","  dec_out, dec_state = decoder_inf([dec_input, dec_state])\n","  # the decoder state is updated and we get the first prediction probability \n","  # vector\n","  decoded_out = tf.argmax(dec_out, axis=-1)\n","  # we decode the softmax vector into and index\n","  pred.append(decoded_out) # update the prediction list\n","  dec_input = decoded_out # the previous pred will be used as the new input\n","\n","pred = tf.concat(pred, axis=-1).numpy()\n","for i in range(10):\n","  print(\"pred:\", pred[i,:])\n","  print(\"true:\", en_val[i,:])\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"e_Rno_pP97hp"},"source":["10. Use the tokenizer to convert the target and predicted sequences back to text, what do you think of the translations?"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1633714558698,"user":{"displayName":"Charles Tanguy","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11930294859591867631"},"user_tz":-120},"id":"tSxu203W3gDC","outputId":"8d9fcf88-f230-4d2a-a1fd-7f82e30a29f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["true: we succeeded\n","pred we need it fit\n","\n","\n","true: nice timing\n","pred nice timing on here\n","\n","\n","true: don't do it\n","pred don't do it out\n","\n","\n","true: anyone home\n","pred anyone home now drink\n","\n","\n","true: it's broken\n","pred it's a book hurt\n","\n","\n","true: i'm punctual\n","pred i'm not home home\n","\n","\n","true: i rewrote it\n","pred i saw him go\n","\n","\n","true: grab my hand\n","pred let's talk to me\n","\n","\n","true: he pinched me\n","pred he has braces in\n","\n","\n","true: i surrender\n","pred i hate dogs drink\n","\n","\n"]}],"source":["y_sample = tokenizer_en.sequences_to_texts(en_val)[:10]\n","pred_sample = tokenizer_en.sequences_to_texts(pred)[:10]\n","\n","for i, j in zip(y_sample,pred_sample):\n","  print(\"true:\", i)\n","  print(\"pred\", j)\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"VviPc59W-pJI"},"source":["11. Now that you reached the end of the exercise, go back to the beginning and increase the number of sentences your model will train on, this should significantly improve the quality of the predictions!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"01-Translator_encoder_decoder.ipynb","provenance":[]},"interpreter":{"hash":"5a55ff060dca2f919002028c6c65853c885111584988b328db325f9c1cd9b339"},"kernelspec":{"display_name":"Python 3.8.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
